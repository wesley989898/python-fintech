{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "#運用網址取得內容\n",
    "def getsoup():   \n",
    "    url = str(row['url'])\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "#爬取網站內容，並依<p>將所有內容合在一起\n",
    "def getcontent(x):\n",
    "    for data in x.find_all('p'):         #將一篇文章中找出有標籤(<p>)的內容\n",
    "        news_content = data.text  \n",
    "        conclude.extend([news_content])  #將所有<p>中的資料合併在一起\n",
    "    content_test = ' '.join(conclude)    #因為會有多個資料，但型態是 '','',''，所以要將這些資料都merge成一個\n",
    "    l6.extend([content_test])            #最後將資料放入l6的陣列中\n",
    "    return l6\n",
    "\n",
    "df = pd.read_excel(\"fintech資料0509_2000.xlsx\", columns = 'post') \n",
    "\n",
    "l6 = [] #新聞內文\n",
    "\n",
    "for i, row in df.iterrows(): \n",
    "    conclude = []\n",
    "    if row['sitename'] == 'UDN': #根據不同網站名稱進行爬蟲\n",
    "        try:\n",
    "            x = getsoup().find(id = 'story')\n",
    "            getcontent(x)\n",
    "            \n",
    "        except:\n",
    "            content = '該頁被刪除'\n",
    "            l6.extend([content])\n",
    "          \n",
    "    elif row['sitename'] == 'Yahoo奇摩股市':\n",
    "        x = getsoup().find(id = 'aritcletable')\n",
    "        getcontent(x)\n",
    "        \n",
    "    elif row['sitename'] == '富聯網': \n",
    "        x = getsoup().find(id = 'NewsMainContent')\n",
    "        getcontent(x)\n",
    "        \n",
    "    elif row['sitename'] == 'Chinatimes':\n",
    "        x = getsoup().find('div', class_ = 'article-body')      \n",
    "        getcontent(x)\n",
    "        \n",
    "    elif row['sitename'] == '工商時報':\n",
    "        x = getsoup().find('div', class_ = 'article-body')      \n",
    "        getcontent(x)\n",
    "        \n",
    "    elif row['sitename'] == 'CNA產業新聞':\n",
    "        x = getsoup().find(id = 'textContent')\n",
    "        l6.extend([x])\n",
    "        \n",
    "    elif row['sitename'] == 'Yam':\n",
    "        x = getsoup().find(class_ = 'newsArticle')      \n",
    "        getcontent(x)\n",
    "        \n",
    "    elif row['sitename'] == 'Smart智富網':\n",
    "        x = getsoup().find(class_ = \"kidssearchfont13\")\n",
    "    \n",
    "        for y in x:\n",
    "            for text in y.find_next_siblings(text=True):\n",
    "                news_content = text.strip()\n",
    "                conclude.extend([news_content])\n",
    "        content_test = ' '.join(conclude)\n",
    "        l6.extend([content_test]) \n",
    "    \n",
    "    elif row['sitename'] == 'Hinet新聞':\n",
    "        try:\n",
    "            x = getsoup().find('div',class_ = 'newsContent')\n",
    "            l6.extend([x.string])\n",
    "        except:\n",
    "            content = '該頁被刪除'\n",
    "            l6.extend([content])\n",
    "            \n",
    "    elif row['sitename'] == 'PChome基金':\n",
    "        url = str(row['url'])  \n",
    "        r = requests.get(url)\n",
    "        r.encoding='UTF-8' #解碼\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        x = soup.find(class_ = \"t1320\") \n",
    "        text = x.find_next_siblings(text=True)\n",
    "        l6.extend([text])\n",
    "        \n",
    "    else:\n",
    "        content = 'NONE'\n",
    "        l6.extend([content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#l1,l2,l3,l4的資料數量都必須一致，才能進行整合!!!\n",
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "df = pd.read_excel(\"fintech資料0509_2000.xlsx\", columns = 'post') \n",
    "\n",
    "l1 = []  #公司名稱\n",
    "l2 = []  #新聞來源\n",
    "l3 = []  #發布時間\n",
    "l4 = []  #新聞標題\n",
    "l5 = []  #新聞url\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    a = row['company']\n",
    "    l1.extend([a])\n",
    "    b = row['sitename']\n",
    "    l2.extend([b])\n",
    "    c = row['time']\n",
    "    l3.extend([c])    \n",
    "    d = row['title']\n",
    "    l4.extend([d])\n",
    "    e = row['url']\n",
    "    l5.extend([e])\n",
    "\n",
    "df = DataFrame({'company': l1, 'sitename': l2, 'time': l3, 'title': l4, 'url': l5, 'content': l6})\n",
    "df.to_excel('新聞內文0509.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
